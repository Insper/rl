\documentclass[a4paper,10pt,portuguese]{article}
\usepackage[latin1]{inputenc}
\usepackage[portuguese]{babel}
%\usepackage{babel}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{epsfig}
\usepackage{alltt,fancyvrb}
\usepackage{listings}
\usepackage{float}
\usepackage{dentitle}
\usepackage{personal}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{ctable}
\usepackage{listings}
\usepackage{subfig}

\usepackage{xcolor}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

\renewcommand*{\lstlistingname}{Exemplo de código}

\renewcommand{\rmdefault}{cmss}

\begin{document}

\title{Avaliação Final}
\author{Prof. Fabrício Barth}
\date{$1^{o}$ semestre de 2024}
\makedendentitle{INSPER}{Reinforcement Learning}{}

\textbf{Nome}:

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,1);
\end{tikzpicture}

\begin{itemize}
\item \emph{Quaisquer hipóteses relevantes devem ser \textbf{explicitamente formuladas}. Faz parte da avaliação da prova a \textbf{correta interpretação} das questões. A \textbf{clareza} e a \textbf{objetividade} das respostas serão consideradas na avaliação.}

\end{itemize}

\begin{enumerate}

\item Considere um agente que atua em um ambiente onde o conjunto de estados é definido por $S = {s_{1}, s_{2}, s_{3}, s_{4}, s_{5}}$, o conjunto de ações por $A = {a_{1}, a_{2}, a_{3}}$ e o espaço de estados é definido pelo grafo abaixo: 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{img/grafo}
	\caption[Espaço de estados do ambiente]{Espaço de estados do ambiente}
	\label{fig:grafo}
\end{figure}


Este mesmo agente foi treinado para atuar neste ambiente usando o algoritmo \textbf{Q-Learning}. 
Ao final do treinamento foi gerada a seguinte \textbf{Q-table}: 

\begin{center}
\begin{tabular}{|c||c|c|c|}
	\hline
	\rule[-1ex]{0pt}{2.5ex}  & $a_{1}$ & $a_{2}$ & $a_{3}$ \\
	\hline
	\hline
	\rule[-1ex]{0pt}{2.5ex} $s_{1}$ & -0.1 & 0.01 & 0.1 \\
	\hline
	\rule[-1ex]{0pt}{2.5ex} $s_{2}$ & 0.02 & 0.03 & 0.01 \\
	\hline
	\rule[-1ex]{0pt}{2.5ex} $s_{3}$ & 0.003 & 0.002 & 0.004\\
	\hline
	\rule[-1ex]{0pt}{2.5ex} $s_{4}$ & 0 & 0 & 0\\
	\hline
	\rule[-1ex]{0pt}{2.5ex} $s_{5}$ & 1 & 0 & -1\\
	\hline
\end{tabular}
\end{center}

\textbf{Considere que o agente inicia no estado $s_{2}$ e que o estado terminal é o $s_{4}$, qual é a sequência de ações que o agente irá executar para chegar ao estado final?}

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,2);
\end{tikzpicture}

\item No ambiente Cliff Walking a função de reward é definida como: 

\begin{equation} 
f(x)=\begin{cases}
	-1, & \text{para cada step}\\
	-100, & \text{se o agente cair no penhasco}.
\end{cases}
\end{equation}	

E o único estado terminal é quando o agente alcança o objetivo. Caso o agente caia no penhasco ele retorna para o estado inicial, mas o episódio não termina.

\textbf{Existe diferença no comportamento de um agente treinado usando o algoritmo Q-Learning e um agente treinado usando SARSA neste ambiente? Justifique a sua resposta.}

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,5);
\end{tikzpicture}

\item O ambiente \textbf{Simple Grid} é um ambiente onde o agente sabe executar 4 ações (cima, baixo, esquerda e direita) em um mapa pré-configurado com obstáculos (figura \ref{fig:simplegrid}). 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{img/simple_grid}
	\caption[Exemplo de ambiente simple grid]{Exemplo de ambiente simple grid}
	\label{fig:simplegrid}
\end{figure}

O agente é treinado sempre no mesmo mapa com as mesmas dimensões e obstáculos. Os únicos dados que mudam são o estado inicial e final. O estado inicial é representado pelo quadrado vermelho e o estado final é representado pelo quadrado verde. A forma como os estados são tratados pelo agente também é muito simples. Por exemplo, para a figura \ref{fig:simplegrid} existem 64 estados possíveis porque o ambiente é um ambiente 8 por 8. Cada estado é representado por um número inteiro. Se o agente está na casa mais a esquerda e no topo do ambiente então o estado é representado pelo número 1. Na figura \ref{fig:simplegrid} o estado final é representado pelo número 64. 

A função de \textbf{reward} é definida por: 

\begin{lstlisting}[language=Python, caption=Função de reward para o ambiente Simple Grid]
def get_reward(self, x: int, y: int) -> float:
	"""
	Get the reward of a given cell.
	"""
	if not self.is_in_bounds(x, y):
		# if the agent tries to exit the grid, it receives a negative reward
		return -1.0
	elif not self.is_free(x, y):
		# if the agent tries to walk over a wall, it receives a negative reward
		return -1.0
	elif (x, y) == self.goal_xy:
		# if the agent reaches the goal, it receives a positive reward
		return 1.0
	else:
		# otherwise, it receives no reward
		return 0.0
\end{lstlisting}

Depois do agente treinado usando o seguinte algoritmo com os seguintes hiperparâmetros: 

\begin{lstlisting}
qlearn = QLearning(
	env, alpha=0.1, gamma=0.999, epsilon=0.8, 
	epsilon_min=0.05, epsilon_dec=0.999, episodes=5000)
\end{lstlisting}

E apresentando o seguinte comportamento ao longo do treinamento (figuras \ref{fig:lcsimplegrid} e \ref{fig:epsilonsimplegrid}). 

\begin{figure}[htpb]
	\centering
	\subfloat[\centering Curva de aprendizado]{{\includegraphics[width=0.45\linewidth]{img/lc_simple_grid}\label{fig:lcsimplegrid}}}%
	\qquad
	\subfloat[\centering Decaimento do $\epsilon$]{{\includegraphics[width=0.45\linewidth]{img/epsilon_simple_grid}\label{fig:epsilonsimplegrid} }}%
	
	\caption{Figuras relacionadas com o aprendizado do agente no ambiente Simple Grid}
	\label{fig:lcsimplegrid}
\end{figure}

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=0.7\linewidth]{img/lc_simple_grid}
%	\caption[Curva de aprendizado]{Curva de aprendizado}
%	\label{fig:lcsimplegrid}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=0.7\linewidth]{img/epsilon_simple_grid}
%	\caption[Decaimento do epsilon]{Decaimento do epsilon}
%	\label{fig:epsilonsimplegrid}
%\end{figure}

Ao terminar o treinamento do agente e usá-lo no ambiente da seguinte forma: 

\begin{lstlisting}[language=Python, caption=Uso do agente no ambiente Simple Grid]
env = gym.make('SimpleGrid-8x8-v0', render_mode='human')
state, infor = env.reset()
done = False
rewards = 0
actions = 0

while not done and actions < 70:
	action = np.argmax(q_table[state])
	state, reward, done, truncated, info = env.step(action)

	rewards = rewards + reward
	actions = actions + 1

print("Actions taken: {}".format(actions))
print("Rewards: {}".format(rewards))
\end{lstlisting}

Ele tem o seguinte resultado: 

\begin{lstlisting}
(venv) ?  simple_grid python TreinandoAgente.py 
Actions taken: 70
Rewards: 0.0

(venv) ?  simple_grid python TreinandoAgente.py
Actions taken: 70
Rewards: 0.0

(venv) ?  simple_grid python TreinandoAgente.py
Actions taken: 70
Rewards: 0.0
\end{lstlisting}

\textbf{Qual é a explicação?}

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,8);
\end{tikzpicture}

\item O ambiente \textit{Frozen Lake} é um exemplo de ambiente não determinístico que vimos nesta disciplina. Ao treinarmos um agente para atuar neste ambiente usando a seguinte configuração:

\begin{lstlisting}
env = gym.make('FrozenLake-v1', render_mode='ansi').env
qlearn = QLearning(env, alpha=0.1, gamma=0.99, epsilon=0.8, epsilon_min=0.01, epsilon_dec=0.999, episodes=20_000)
\end{lstlisting}

temos uma curva de aprendizado como a apresentada na figura \ref{fig:learning_frozen}, onde a linha pontilhada significa uma meta a ser atingida. 

\begin{figure}[H]
	\centering
	\subfloat[\centering Curva de aprendizado do agente para o ambiente Frozen Lake]{{\includegraphics[width=0.45\linewidth]{img/lc_frozen}\label{fig:learning_frozen}}}%
	\qquad
	\subfloat[\centering Segunda versão da curva de aprendizado do agente para o ambiente Frozen Lake]{{\includegraphics[width=0.45\linewidth]{img/lc_frozen_aleatorio}\label{fig:lcfrozenaleatorio} }}%
	
	\caption{Figuras relacionadas com o aprendizado do agente no ambiente Frozen Lake}
	\label{fig:frozen}
\end{figure}

Além disso, depois de testar o agente treinado usando uma rotina com 100 testes que executam 100 episódios no ambiente, temos uma \textbf{média de $78.56$ vezes que o agente consegue chegar ao destino, com um desvio padrão de $4.73$}.

Ao mudarmos as configurações para: 

\begin{lstlisting}
	env = gym.make('FrozenLake-v1', render_mode='ansi').env
	qlearn = QLearning(env, alpha=0.1, gamma=0.99, epsilon=0.8, epsilon_min=0.01, epsilon_dec=1, episodes=20_000)
\end{lstlisting} 

temos uma curva de aprendizado como a apresentada na figura \ref{fig:lcfrozenaleatorio}. Mas ao testar o agente treinado, usando o mesmo método descrito acima, temos \textbf{uma média de $81.99$ vezes que o agente consegue chegar ao destino, com um desvio padrão de $3.75$}.

\textbf{Explique o que aconteceu.}

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,8);
\end{tikzpicture}

\item Os algoritmos Deep Q-learning ou Deep Q-networks (DQN) são algoritmos que substituem a Q-table por uma rede neural. Talvez a arquitetura de rede neural mais utilizada e com certeza a mais simples é a \textit{full-connected}, muitas vezes chamada de \textit{multilayer perceptron} (MLP). Estas redes geralmente têm uma camada de entrada, uma ou duas camadas intermediárias e uma camada de saída. O que difere entre uma implementação e outra é a quantidade de nodos de entrada, quantidade de nodos nas camadas intermediárias e quantidade de nodos na camada de saída. A quantidade de nodos nas camadas intermediárias é uma quantidade definida de forma empirica, mas que dependende da quantidade de nodos na primeira e última camadas. \textbf{O que define a quantidade de nodos na primeira camada da MLP e o que define a quantidade de nodos na última camada da MLP?} 

%Utilize como exemplos na sua resposta os ambientes \textbf{Cart Pole} e \textbf{Lunar Lander}. No ambiente \textbf{Cart Pole} o observation space é um vetor de tamanho 4 e o action space é discreto com duas ações. No ambiente \textbf{Lunar Lander} o observation space é um vetor de tamanho 8 e o action space é discreto com quatro ações.  

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,8);
\end{tikzpicture}

\item Todos os algoritmos da família Deep Q-Learning tem o conceito de \textit{experience replay}. O tamanho deste \textit{experience replay} é definido por um hiperparâmetro. Na implementação do DQN na biblioteca \textit{stable baseline} este hiperparâmetro se chama \textit{replay buffer}. Na figura \ref{fig:analise_replay_buffer} é possível ver várias curvas de aprendizado para agentes atuando no ambiente \textit{Lunar Lander}.  

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/analise_replay_buffer_dqn}
	\caption[Análise do impacto do replay buffer no aprendizado do agente]{Análise do impacto do replay buffer no aprendizado do agente}
	\label{fig:analise_replay_buffer}
\end{figure}

Estes agentes foram treinados usando os hiperparâmetros padrão do DQN variando apenas o $buffer\_size$ $D = [1, 10^{2}, 10^{3}, 10^{4}, 10^{5}, 10^{6}]$. O objetivo desta análise é verificar o impacto do tamanho do \textit{experience replay} no treinamento do agente. 

\textbf{A partir da análise do gráfico disponível em \ref{fig:analise_replay_buffer}, quais são as conclusões que podemos chegar? Por que o desempenho do agente com $buffer\_size = 1$ é tão diferente dos demais?}

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,8);
\end{tikzpicture}

\item O Algoritmo Double Deep Q-Learning, além de receber os hiperparâmetros normalmente utilizados no algoritmo Q-Learning ($\alpha$, $\gamma$, $\epsilon$, $\epsilon_{min}$, $\epsilon_{dec}$, quantidade de episódios), também tem como hiperparâmetros: tamanho da memória do \textit{experience replay}, \textit{batch size} e o tamanho do intervalo para atualização da \textit{target network}. 

Considere uma implementação que faz uso da configuração de hiperparâmetros e da rede neural definidos no código \ref{lst:hiper} e no código \ref{lst:rede}, respectivamente. 

\begin{lstlisting}[language=Python, caption=Lista de hiperparâmetros, label=lst:hiper]
gamma = 0.99 
epsilon = 1.0
epsilon_min = 0.01
epsilon_dec = 0.99
episodes = 1000
batch_size = 64
learning_rate=0.001
memory = deque(maxlen=10000)
ma_steps = 1500	
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Denifição da Rede Neural, label=lst:rede]
model = Sequential()
model.add(Dense(512, activation=relu, input_dim=env.observation_space.shape[0]))
model.add(Dense(256, activation=relu))
model.add(Dense(env.action_space.n, activation=linear))
\end{lstlisting}

O algoritmo DQN está disponível na biblioteca \textbf{stable\_baselines3}. O construtor deste algoritmo recebe os seguintes parâmetros: 

\begin{lstlisting}[language=Python, caption=Parâmetros da classe DQN, label=lst:dqn]
class stable_baselines3.dqn.DQN(
	policy, 
	env, 
	learning_rate=0.0001, 
	buffer_size=1000000, 
	batch_size=32, 
	tau=1.0, 
	gamma=0.99, 
	optimize_memory_usage=False, 
	target_update_interval=10000, 
	exploration_fraction=0.1, 
	exploration_initial_eps=1.0, 
	exploration_final_eps=0.05) 
\end{lstlisting}

onde o parâmetro \textit{policy} pode receber três valores possíveis: \textit{MlpPolicy}, \textit{CnnPolicy} ou \textit{MultiInputPolicy}.

\textbf{Considerando o código em \ref{lst:hiper}, \ref{lst:rede} e \ref{lst:dqn}, complete o código abaixo:}

\begin{lstlisting}
env = gym.make("LunarLander-v2")

model = DQN(
policy=______________________,
env=env,
__________
"outros parâmetros que você julgar relevante"
_________
)
\end{lstlisting} 

Utilize os hiperparâmetros definidos em \ref{lst:hiper} e a rede definida em \ref{lst:rede} para definir uma nova instância de DQN. Escreva o código abaixo e justifique a sua tomada de decisão: 

\begin{tikzpicture}
	\draw[blue, very thick] (0,0) rectangle (16,12);
\end{tikzpicture}

\end{enumerate}

%\bibliographystyle{plain} % estilo da ABNT
%\bibliography{doutorado,mestrado} % arquivo bibtex

\end{document}










