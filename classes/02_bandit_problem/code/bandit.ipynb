{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755dbb26",
   "metadata": {},
   "source": [
    "# Bandit, Exploration and Exploitation\n",
    "\n",
    "The goal of this exercise is to implement a simple bandit algorithm and test it on a simple environment. \n",
    "\n",
    "We will first start by understanding the problem. \n",
    "\n",
    "## 1. Understanding the Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe5d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0 - Reward: 8.712120281027941\n",
      "Action: 2 - Reward: 3.2208998151730412\n",
      "Action: 2 - Reward: 1.830416837274484\n",
      "Action: 1 - Reward: 2.2391969627753454\n",
      "Action: 2 - Reward: 3.5057851711861545\n",
      "Action: 1 - Reward: -2.1048408678210935\n",
      "Action: 2 - Reward: 2.3880164249171147\n",
      "Action: 0 - Reward: 9.584807438315394\n",
      "Action: 1 - Reward: 0.8030161050395389\n",
      "Action: 2 - Reward: 3.249491480681207\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import buffalo_gym\n",
    "\n",
    "env = gym.make(\"Buffalo-v0\", arms=3)\n",
    "obs = env.reset()\n",
    "count = 0\n",
    "while count < 10:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action} - Reward: {reward}\")\n",
    "    count += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6bf07",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "1. What the code is doing?\n",
    "2. What is the best option to take?\n",
    "3. What is the expected reward of taking the best option?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf6d0f",
   "metadata": {},
   "source": [
    "## 2. Implementing an Incremental Update Rule\n",
    "\n",
    "Complete the function `incremental_update` to implement the incremental update rule for the action-value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_update(Q, Times,action, reward):\n",
    "    \"\"\"\n",
    "    Update the action-value estimate Q for the given action and reward using an incremental update rule.\n",
    "\n",
    "    Parameters:\n",
    "    Q (list): A list of action-value estimates for each action.\n",
    "    Times (list): A list of counts of how many times each action has been taken.\n",
    "    action (int): The index of the action taken.\n",
    "    reward (float): The reward received after taking the action.\n",
    "\n",
    "    Returns:\n",
    "    list: Updated list of action-value estimates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e8833",
   "metadata": {},
   "source": [
    "And execute the following code to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import buffalo_gym\n",
    "\n",
    "arms = 10\n",
    "Q = [0.0 for _ in range(arms)]\n",
    "Times = [0 for _ in range(arms)]\n",
    "env = gym.make(\"Buffalo-v0\", arms=arms)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    #print(f\"Action: {action} - Reward: {reward}\")\n",
    "    Times[action] += 1\n",
    "    Q = incremental_update(Q, Times, action, reward)\n",
    "    done = terminated or truncated\n",
    "env.close()\n",
    "\n",
    "print(\"Final action-value estimates:\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd73ec",
   "metadata": {},
   "source": [
    "Questions: \n",
    "1. Which is the best action?\n",
    "2. What is the expected reward of taking the best action?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58728f06",
   "metadata": {},
   "source": [
    "## 3. Greedy Action Selection\n",
    "\n",
    "Now that we have implemented the incremental update rule, we can implement a greedy action selection strategy.\n",
    "Complete the function `greedy_action_selection` to implement a greedy action selection strategy.\n",
    "This function will be replace the instruction `action = env.action_space.sample()` in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cced670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We don't want to use the argmax function from numpy because it doesn't break ties randomly. \n",
    "# We want to implement our own version of argmax that breaks ties randomly.\n",
    "\n",
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        \n",
    "        # your code here!\n",
    "        ties = ties # replace this with your code to update ties\n",
    "        \n",
    "    # return a random selection from ties.\n",
    "    return np.random.choice(ties) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75764129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Debugging Cell\n",
    "# --------------\n",
    "# Feel free to make any changes to this cell to debug your code\n",
    "\n",
    "test_array = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "assert argmax(test_array) == 8, \"Check your argmax implementation returns the index of the largest value\"\n",
    "\n",
    "# make sure np.random.choice is called correctly\n",
    "np.random.seed(0)\n",
    "test_array = [1, 0, 0, 1]\n",
    "\n",
    "assert argmax(test_array) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7293b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More testing to make sure argmax does not always choose first entry\n",
    "\n",
    "test_array = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "assert argmax(test_array) == 8, \"Check your argmax implementation returns the index of the largest value\"\n",
    "\n",
    "# set random seed so results are deterministic\n",
    "np.random.seed(0)\n",
    "test_array = [1, 0, 0, 1]\n",
    "\n",
    "counts = [0, 0, 0, 0]\n",
    "for _ in range(100):\n",
    "    a = argmax(test_array)\n",
    "    counts[a] += 1\n",
    "\n",
    "# make sure argmax does not always choose first entry\n",
    "assert counts[0] != 100, \"Make sure your argmax implementation randomly choooses among the largest values.\"\n",
    "\n",
    "# make sure argmax does not always choose last entry\n",
    "assert counts[3] != 100, \"Make sure your argmax implementation randomly choooses among the largest values.\"\n",
    "\n",
    "# make sure the random number generator is called exactly once whenver `argmax` is called\n",
    "expected = [44, 0, 0, 56] # <-- notice not perfectly uniform due to randomness\n",
    "assert counts == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_action_selection(Q):\n",
    "    \"\"\"\n",
    "    Select an action using a greedy action selection strategy based on the action-value estimates Q.\n",
    "\n",
    "    Parameters:\n",
    "    Q (list): A list of action-value estimates for each action.\n",
    "\n",
    "    Returns:\n",
    "    int: The index of the selected action.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9214a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this version, we will run 1000 steps of the \n",
    "# bandit problem and calculate the accumulated reward at each step.\n",
    "# We will run 100 times and average the rewards over time to see if \n",
    "# the agent is learning to select the best action.\n",
    "\n",
    "import gymnasium as gym\n",
    "import buffalo_gym\n",
    "\n",
    "arms = 10\n",
    "steps = 1000\n",
    "runs = 2000\n",
    "\n",
    "average_rewards = np.zeros(steps)\n",
    "\n",
    "for run in range(runs):\n",
    "\n",
    "    Q = [0.0 for _ in range(arms)]\n",
    "    Times = [0 for _ in range(arms)]\n",
    "\n",
    "    Rewards = [0.0 for _ in range(steps)]\n",
    "    env = gym.make(\"Buffalo-v0\", arms=arms)\n",
    "\n",
    "    obs = env.reset()\n",
    "    step = 0\n",
    "    while step < steps:\n",
    "        action = greedy_action_selection(Q)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #print(f\"Action: {action} - Reward: {reward}\")\n",
    "        Times[action] += 1\n",
    "        Q = incremental_update(Q, Times, action, reward)\n",
    "        Rewards[step] = reward\n",
    "        step += 1\n",
    "\n",
    "    env.close()\n",
    "    #print(\"Final action-value estimates:\", Q)\n",
    "    average_rewards += np.array(Rewards)\n",
    "\n",
    "average_rewards /= runs\n",
    "\n",
    "# plot the average rewards over steps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_rewards)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average Reward over Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4f916",
   "metadata": {},
   "source": [
    "Questions: \n",
    "1. Is the agent able to find the best action?\n",
    "2. Are the rewards improving over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaafdbc",
   "metadata": {},
   "source": [
    "## 4. Epsilon-Greedy Action Selection\n",
    "\n",
    "Now that we have implemented a greedy action selection strategy, we can implement an epsilon-greedy action selection strategy.\n",
    "Complete the function `epsilon_greedy_action_selection` to implement an epsilon-greedy action selection strategy.\n",
    "This function will be replace the instruction `action = greedy_action_selection(Q)` in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(Q, epsilon):\n",
    "    \"\"\"\n",
    "    Select an action using an epsilon-greedy action selection strategy based on the action-value estimates Q.\n",
    "\n",
    "    Parameters:\n",
    "    Q (list): A list of action-value estimates for each action.\n",
    "    epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "\n",
    "    Returns:\n",
    "    int: The index of the selected action.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this version, we will run 1000 steps of the \n",
    "# bandit problem and calculate the accumulated reward at each step.\n",
    "# We will run 100 times and average the rewards over time to see if \n",
    "# the agent is learning to select the best action.\n",
    "\n",
    "import gymnasium as gym\n",
    "import buffalo_gym\n",
    "\n",
    "arms = 10\n",
    "steps = 1000\n",
    "runs = 2000\n",
    "\n",
    "average_rewards_ep = [0.0 for _ in range(steps)]\n",
    "\n",
    "for run in range(runs):\n",
    "\n",
    "    Q = [0.0 for _ in range(arms)]\n",
    "    Times = [0 for _ in range(arms)]\n",
    "\n",
    "    Rewards = [0.0 for _ in range(steps)]\n",
    "    env = gym.make(\"Buffalo-v0\", arms=arms)\n",
    "\n",
    "    obs = env.reset()\n",
    "    step = 0\n",
    "    while step < steps:\n",
    "        action = epsilon_greedy_action_selection(Q, epsilon=0.1)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #print(f\"Action: {action} - Reward: {reward}\")\n",
    "        Times[action] += 1\n",
    "        Q = incremental_update(Q, Times, action, reward)\n",
    "        Rewards[step] = reward\n",
    "        step += 1\n",
    "\n",
    "    env.close()\n",
    "    #print(\"Final action-value estimates:\", Q)\n",
    "    average_rewards_ep += np.array(Rewards)\n",
    "\n",
    "average_rewards_ep /= runs\n",
    "\n",
    "# plot the average of Rewards over steps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_rewards_ep)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average Reward over Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d910f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both greedy and epsilon-greedy rewards over steps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_rewards, label=\"Greedy\")\n",
    "plt.plot(average_rewards_ep, label=\"Epsilon-Greedy e=0.1\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average Reward over Steps\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
