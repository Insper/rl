# Value-based vs Policy-based Methods

Em aprendizagem por refor√ßo (RL), as abordagens policy-based e value-based diferem no foco e no modo de aprender a estrat√©gia √≥tima do agente. 

Nos m√©todos do tipo value-based, o agente aprende uma fun√ß√£o de valor (ex.: $ùëÑ(ùë†,ùëé)$) que estima ‚Äúo qu√£o bom‚Äù √© estar em um estado ou tomar uma a√ß√£o. A pol√≠tica √© derivada indiretamente, escolhendo sempre a a√ß√£o com maior valor estimado ($policy = \arg\max_{a} Q(s,a)$). Exemplos de algoritmos que utilizam esta estrat√©gia s√£o Q-Learning e Deep Q-Learning (DQN). Esta abordagem √© mais *sample-efficient* (aproveita melhor cada experi√™ncia) e funciona bem em espa√ßo de a√ß√µes discreto e pequeno. Em contrapartida, tem dificuldade com a√ß√µes cont√≠nuas (o $\arg\max$ fica invi√°vel) e tamb√©m pode ficar inst√°vel ao aproximar fun√ß√µes de valor com redes neurais. 

Nos m√©todos do tipo policy-based, o agente aprende diretamente uma pol√≠tica ($\pi_{\theta}(a|s)$). Dado um estado $s$, a pol√≠tica $\theta$ fornece uma probabilidade maior para a√ß√µes $a$ que levam a maiores recompensas. Ou seja, o agente mapeia estados para a√ß√µes, sem a necessidade de uma fun√ß√£o de valor intermedi√°ria. A pol√≠tica √© geralmente parametrizada (ex.: por uma rede neural) e otimizada diretamente para maximizar a recompensa esperada. Exemplo de algoritmo que utiliza esta estrat√©gia √© o REINFORCE. Esta abordagem lida melhor com espa√ßos de a√ß√£o cont√≠nuos e pode aprender pol√≠ticas estoc√°sticas. No entanto, tende a ser menos *sample-efficient*.





