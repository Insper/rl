# Coment√°rios sobre as implementa√ß√µes do Q-Learning e Sarsa

[An√°lise dos treinamentos do Q-Learning e Sarsa](./analise_treinamento.html)

Agente treinado com Q-Learning no ambiente CliffWalking:

<img src="figures/QLearning-Cliff.png" alt="Q-Learning in the Cliff walking world" style="height: 200px;"/>

Agente treinado com Sarsa no ambiente CliffWalking:

<img src="figures/Sarsa-Cliff.png" alt="Sarsa in the Cliff walking world" style="height: 200px;"/>

## QLearning vs  Sarsa - Vantagens e Desvantagens üìåÔ∏è 

- `QLearning`

$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma max(Q(S_{t+1}, a)) - Q(S_t, A_t) ]$$

Algor√≠timo que busca encontrar a melhor a√ß√£o a ser tomada, dado um estado atual. 

√â considerao um algor√≠itmo **off-policy**, pois a melhor a√ß√£o √© escolhida para a atualiza√ß√£o da **q_table** mesmo que essa a√ß√£o n√£o seja aplicada nessa ocasi√£o (fator de aleatoriedade na tomada de uma a√ß√£o - **Explore**).

Como o QLearning aprende com a "pol√≠tica √≥tima", ele √© considerado um algor√≠timo mais "agressivo". Ou seja, No exemplo do ambiente CliffWalking o QL seguir√° o caminho mais curto, pois esse √© o caminho √≥timo, mesmo que haja risco maior de queda.

- `Sarsa`

$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]$$

Como √© poss√≠vel observar pela formula, o algor√≠timo SARSA aprende com uma pol√≠tica "quase √≥tima". Um agente treinado com o algor√≠timo SARSA interage com o ambiente atualizando **q_table** com base nas a√ß√µes efetivamente tomadas. Quando o problema envolve achar a solu√ß√£o √≥tima ou quando o n√∫mero m√≠nimo de a√ß√µes deve ser tomada na resolu√ß√£o do problema, o algor√≠timo SARSA pode n√£o se apresentar como a melhor escolha.

