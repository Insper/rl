
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Fabricio Barth">
      
      
        <link rel="canonical" href="https://insper.github.io/rl/classes/01_introduction/introduction_rl/">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.10">
    
    
      
        <title>Introduction to Reinforcement Learning - Reinforcement Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.0d440cfe.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.2505c338.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/custom.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ai-definitions-and-key-concepts" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Reinforcement Learning" class="md-header__button md-logo" aria-label="Reinforcement Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Introduction to Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/Insper/rl/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Reinforcement Learning
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Reinforcement Learning" class="md-nav__button md-logo" aria-label="Reinforcement Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Reinforcement Learning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Insper/rl/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Reinforcement Learning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../goals/" class="md-nav__link">
        Goals
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../plan/" class="md-nav__link">
        Plan
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../assessment/" class="md-nav__link">
        Student Assessment
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Classes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Classes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Introduction to Reinforcement Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_problem_solving/" class="md-nav__link">
        Problem-solving searching review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../03_games/" class="md-nav__link">
        Adversarial search and games review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../03_review/" class="md-nav__link">
        Implementations review
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../04_toolings_envs/" class="md-nav__link">
        Reinforcement Learning Tooling and Environments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../05_q_learning/" class="md-nav__link">
        Q-Learning Algorithm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../05_x_hyperparameters/" class="md-nav__link">
        Hyperparameters in Q-Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../05_x_sarsa/" class="md-nav__link">
        SARSA Algorithm: on-policy approach
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../05_xx_comments/" class="md-nav__link">
        Comments about Q-Learning and Sarsa implementation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../06_non_determ/" class="md-nav__link">
        Using RL in non-deterministic environments
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../06_non_determ_comments/" class="md-nav__link">
        Comments about the Frozen Lake implementations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../07_game_env/" class="md-nav__link">
        Using RL in a competitive environment
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#autonomous-agent" class="md-nav__link">
    Autonomous Agent
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ibm-deep-blue-versus-kasparov-1997" class="md-nav__link">
    IBM Deep Blue versus Kasparov (1997)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#handwritten-digits-recognition" class="md-nav__link">
    Handwritten digits recognition
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autonomous-car" class="md-nav__link">
    Autonomous car
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imaginenet-dataset-2009" class="md-nav__link">
    ImagineNet dataset (2009)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#alphago-playing-go-2016" class="md-nav__link">
    AlphaGO playing GO (2016)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generative-models-dall-e-and-chatgpt" class="md-nav__link">
    Generative models (DALL-E and chatGPT)
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  



<h1 id="ai-definitions-and-key-concepts">AI Definitions and key concepts</h1>
<h2 id="autonomous-agent">Autonomous Agent</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}</p>
<div class="arithmatex">\[\begin{figure}
\centering
\includegraphics[width=1\textwidth]{figures/AE_loop_without_reward.png}
\end{figure}\]</div>
<p>:::
::: {.column width="50%"}</p>
<p>"Autonomous agents are computational systems. that inhabit some <strong>complex dynamic environment</strong>, <strong>sense</strong> and <strong>act</strong> autonomously in this environment, and by doing so realize a set of <strong>goals</strong> or <strong>tasks</strong> for which they are designed." Pattie Maes</p>
<p>:::
::::::::::::::</p>
<h2 id="ibm-deep-blue-versus-kasparov-1997">IBM Deep Blue versus Kasparov (1997)</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}</p>
<div class="arithmatex">\[\begin{center}
\includegraphics[width=1\textwidth]{figures/deep_blue.jpg}
\end{center}\]</div>
<p>:::
::: {.column width="50%"}</p>
<ul>
<li><strong>environment</strong>: chess board;</li>
<li><strong>task</strong>: play and win a chess game;</li>
<li><strong>actions</strong>: move chess pieces;</li>
<li><strong>implementation</strong>: Min-Max algorithm + heuristics + knowledge base + dedicated hardware.</li>
</ul>
<p>:::
::::::::::::::</p>
<h2 id="handwritten-digits-recognition">Handwritten digits recognition</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}
\includegraphics[width=1\textwidth]{figures/mnist.jpeg}
:::
::: {.column width="50%"}</p>
<p>The <strong>MNIST database</strong> of handwritten digits (1998)</p>
<ul>
<li><strong>environment</strong>: an image</li>
<li><strong>task</strong>: classify an image without error</li>
<li><strong>actions</strong>: classify an image</li>
<li><strong>implementation</strong>: there are a lot of implementations. However, the best approach is always using a neural network model</li>
</ul>
<p>In 2012 a solution achieved a very good result - a test error equals to 0.23% <a href="http://yann.lecun.com/exdb/mnist/">[http://yann.lecun.com/exdb/mnist/]</a></p>
<p>:::
::::::::::::::</p>
<h2 id="autonomous-car">Autonomous car</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}</p>
<div class="arithmatex">\[\begin{center}
\includegraphics[width=1\textwidth]{figures/car_stanford.jpg}
\end{center}\]</div>
<p>:::
::: {.column width="50%"}</p>
<p>This is the car from the Stanford team, which won the DARPA competition in 2005.</p>
<ul>
<li><strong>environment</strong>: a road in a desert;</li>
<li><strong>task</strong>: drive through a desert and get to a specific point;</li>
<li><strong>actions</strong>: speed up, brake, turn left, turn right;</li>
<li><strong>implementation</strong>: a very complex implementation with different sensors and actuators.</li>
</ul>
<p>The Stanford team was the first team that won this competition (2005).</p>
<p>:::
::::::::::::::</p>
<h2 id="imaginenet-dataset-2009">ImagineNet dataset (2009)</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}
\includegraphics[width=1\textwidth]{figures/image_classification.png}
:::
::: {.column width="50%"}</p>
<p>This dataset has more than 14 million images annotated according to <a href="http://wordnet.princeton.edu/">WordNet</a> taxonomy.</p>
<p>This dataset has been used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) since 2010. This competition is a benchmark for <strong>image classification</strong> tasks and <strong>object recognition</strong> tasks.</p>
<p>:::
::::::::::::::</p>
<!-- ## IBM Watson vence no Jeopardy (2011)

\begin{center}
\includegraphics[width=0.5\textwidth]{figures/watson.jpg}
\end{center}

A partir de **2015** proliferação de **assistentes virtuais** que fazem uso de **classificação de texto** para compreensão das intenção de uma sentença.

-->

<h2 id="alphago-playing-go-2016">AlphaGO playing GO (2016)</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}
\includegraphics[width=1\textwidth]{figures/go.jpg}
:::</p>
<p>::: {.column width="50%"}</p>
<ul>
<li><strong>environment</strong>: GO board;</li>
<li><strong>task</strong>: play GO game and win;</li>
<li><strong>actions</strong>: move pieces;</li>
<li><strong>implementation</strong>: reinforcement learning implementation.</li>
</ul>
<p>:::
:::::::::::::: </p>
<h2 id="generative-models-dall-e-and-chatgpt">Generative models (DALL-E and chatGPT)</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}</p>
<p><strong>DALL-E</strong> input: "an autonomous robot solving a problem"</p>
<ul>
<li><strong>environment</strong>: a form where the user can input a text;</li>
<li><strong>task</strong>: generate images that are related to the text informed;</li>
<li><strong>actions</strong>: generate images;</li>
<li><strong>implementation</strong>: a Deep Neural Network</li>
</ul>
<p>::: </p>
<p>::: {.column width="50%"}
\includegraphics[width=1\textwidth]{figures/dall_example.png}</p>
<p>:::
::::::::::::::</p>
<h1 id="reinforcement-learning-definition-and-key-concepts">Reinforcement Learning: definition and key concepts</h1>
<h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2>
<p>:::::::::::::: {.columns}
::: {.column width="50%"}</p>
<div class="arithmatex">\[\begin{figure}
\centering
\includegraphics[width=1\textwidth]{figures/AE_loop.png}
\end{figure}\]</div>
<p>:::
::: {.column width="50%"}</p>
<ul>
<li>
<p>RL is a popular approach to AI where an agent learns to take sequential actions in an environment through trial and error. </p>
</li>
<li>
<p>Each action changes the environment and has a <strong>reward</strong> (positive, negative or neutral).</p>
</li>
<li>
<p>An Agent <strong>learns</strong> a <strong>policy</strong> that maximizes the reward.  </p>
</li>
</ul>
<p>:::
::::::::::::::</p>
<h2 id="examples-12">Examples (&frac12;)</h2>
<ul>
<li>
<p><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>
</li>
<li>
<p><a href="https://www.nature.com/articles/nature14236">Human-Level control through deep reinforcement learning</a>, 2015. <span class="arithmatex">\(\rightarrow\)</span> <em>Overpassed human results</em></p>
</li>
<li>
<p><a href="https://www.science.org/doi/epdf/10.1126/science.aar6404">A General reinforcement learning algorithm that masters chess, shogi, and Go through self-play</a>, 2018. </p>
</li>
</ul>
<h2 id="examples-22">Examples (2/2)</h2>
<ul>
<li>
<p><strong>Self-driving cars</strong>: there are some components in self-driving cars that are optimized through RL.</p>
</li>
<li>
<p><strong>Industry automation</strong>: Google Data Centers are using RL to reduce energy spending. </p>
</li>
<li>
<p><strong>Trading and finance</strong>: there are a lot of companies saying that they are using RL to create robots for trading. </p>
</li>
<li>
<p><strong>Natural Language Processing (NLP)</strong>: chatGTP is using RL to improve its training. </p>
</li>
<li>
<p><strong>Recommmendation</strong>: there are a lot of papers about RL in recommendation systems. </p>
</li>
</ul>
<h1 id="differences-between-other-machine-learning-and-ai-techniques">Differences between other machine learning and AI techniques</h1>
<h2 id="differences">Differences</h2>
<ul>
<li>
<p><strong>Supervised learning</strong>: </p>
<ul>
<li>the main goal of supervised learning is to create a predicted model. </li>
<li>All the model construction is based on training and test datasets and both datasets must have a <strong>label</strong> attribute. </li>
<li>All the training process is in <strong>batch</strong> mode.</li>
</ul>
</li>
<li>
<p><strong>Unsupervised learning</strong>: </p>
<ul>
<li>the main goal of unsupervised learning is to summarize data. Usually, through clustering or rule models.</li>
<li>All the model construction is based on training datasets without a label attribute. </li>
</ul>
</li>
<li>
<p><strong>Reinforcement Learning</strong>:</p>
<ul>
<li>All the learning process is <strong>interactive</strong>. </li>
<li>There is no training data. However, there is an <strong>environment</strong>. </li>
</ul>
</li>
</ul>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 7, 2023</span>
      
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.db81ec45.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6df46069.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/js-yaml/4.0.0/js-yaml.min.js"></script>
      
        <script src="../../../js/markdown-enhancer.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>